from re import *

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    res_string = text.replace('\\t', " ").replace('\\r', " ").replace('\\f', " ")
    if casefold:
        res_string = text.casefold()
    if yo2e:
        res_string = res_string.replace('Ñ‘', 'Ðµ')
        res_string = res_string.replace('Ð', 'Ð•')

    words = res_string.split()
    res_string = ''
    for i in words:
        res_string += i + " "
    res_string = res_string.strip()
    return res_string

# print(f'"ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t" -> {normalize("ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t", casefold = True, yo2e= True)}')
# print(f'"Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°" -> {normalize("Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°", casefold= True, yo2e=True)}')
# print(f'"Hello\r\nWorld" -> {normalize("Hello\r\nWorld", casefold=True)}')
# print(f'"  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  " -> {normalize("  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  ")}')

def tokenize(text: str) -> list[str]:
    return findall(r'\w+(?:-\w+)*', text)

# print(f'"Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€" -> {tokenize("Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€")}')
# print(f'"hello,world!!!" -> {tokenize("hello,world!!!")}')
# print(f'"Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾" -> {tokenize("Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾")}')
# print(f'"2025 Ð³Ð¾Ð´" -> {tokenize("2025 Ð³Ð¾Ð´")}')
# print(f'"emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾" -> {tokenize("emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾")}')

def count_freq(data):
    new_data = []
    new_data = set(data)
    dict = {}
    for i in new_data:
        col = data.count(i)
        dict[i] = col
    return dict

def top_n(dict, n_top):
    text = []
    for key, value in dict.items():
        text.append((key, value))
    ans = text.sort()
    ans = sorted(text, key = lambda x: (x[1]), reverse=True)
    res = []
    if n_top > len(dict):
        for i in range(len(dict)):
            res.append(ans[i])
    else:
        for i in range(n_top):
            res.append(ans[i])
    return res

# print(f'tokens ["a","b","a","c","b","a"] -> {count_freq(["a","b","a","c","b","a"])}')
# print(f'tokens ["a","b","a","c","b","a"] -> {top_n(count_freq(["a","b","a","c","b","a"]), 2)}')
# print(f'tokens ["bb","aa","bb","aa","cc"] -> {count_freq(["bb","aa","bb","aa","cc"])}')
# print(f'tokens ["bb","aa","bb","aa","cc"] -> {top_n(count_freq(["bb","aa","bb","aa","cc"]), 2)}')